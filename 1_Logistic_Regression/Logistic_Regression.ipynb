{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression (and some Linear Regression)\n",
    "#### Dennis Bakhuis - 14th April 2020\n",
    "https://linkedin.com/in/dennisbakhuis/ \\\n",
    "https://github.com/dennisbakhuis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents\n",
    "1. A short note on why Logistic Regression is a building block for Deep Learning\n",
    "2. One step back: Linear Regression\n",
    "    1. Short recap on linear regression\n",
    "    2. An implementation in Tensorflow\n",
    "    3. What's actually happening 'under the hood'\n",
    "3. From Linear Regression to Binary Logistic Regression\n",
    "    1. What is the difference?\n",
    "    2. An implementation in Tensorflow\n",
    "    3. What's actually happening\n",
    "4. Round up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) A short note on why Logistic Regression is a building block for Deep Learning\n",
    "When we hear or read about *deep learning* we generally mean the subfield of machine learning using *artificial neural networks* (ANN). These computing systems are quite succesful in solving complex problems in various fields, examples are, image recognition, language modelling, and speech recognition. While the name ANN implies that they are related to the inner workings of our brain, the truth is that they mainly share some terminology. An ANN generally consists of multiple interconnected layers, which on itself is build using neurons (also called nodes). An typical example is shown below (downloaded from https://dlpng.com/png/6805993):\n",
    "\n",
    "<img src=\"assets/neuralnetwork.png\" alt=\"Artificial Neural Network example\" width=\"600\" style=\"display: block; margin: 0 auto\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we have one input layer, consisting of four individual inputs nodes. This input layer is 'fully connected' to the first hidden layer, i.e. Fully connected means that each input is connected to each node. The first hidden layer is again fully connected to another 'hidden' layer. The term hidden indicates that we are not directly interact with these layers and these are kind of obscured to the user. The second hidden layer is on its turn fully connected two the final output layer, which consists of two nodes. So in this example we feed the model four inputs and we will receive two outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now focus on a single neuron from our previous example. This neuron still is connected to all inputs, which are also called features. Using these features, the neuron calculates a single response (or output). A diagram of such a system looks like this (my own creativity):\n",
    "\n",
    "<img src=\"assets/logisticunit.png\" alt=\"Image\" width=\"600\" style=\"display: block; margin: 0 auto\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input features are named $f^1$, $f^2$, $f^3$, and $f^4$ and are all connected to our single neuron. This neuron executes two operations. First, a multiplication of each feature with a weight($W$) and adding a bias ($b$). The second and final operation is a so called *activation function*, indicated by $\\sigma$. This results in a probability between zero and one as the output. A single neuron acts like a small logistic regression model and therefore, an ANN can be seen as a bunch of interconnected logistic regression models stacked together. While this idea is pretty neat, the underlying truth is a bit more subtle. There are many different architectures for ANNs and they can use various building blocks that act qute different than in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first operation of the neuron is a linear function, and is nothing more than a linear regression. Therefore, to understand logistic regression, the first step is to have an idea for the linear regression step of our single neuron. Let's do a recap in the next section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) One step back: Linear Regression\n",
    "### 2A) Short recap on linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression in its simplest form (also called simple linear regression), models a single dependent variable $y$ using a single independent variable $x$. This may sound daunting, but was this means is that we want to solve the following equation:\n",
    "\n",
    "\\begin{equation}\n",
    "y = w x + b\n",
    "\\end{equation}\n",
    "\n",
    "In the context of *machine learning*, $x$ represents our input data, $y$ represents our output data, and by solving we mean to find the best weights (or parameters), which are $w$ and $b$ in this equation. A computer can help us find the best values for $w$ and $b$, to have the closest match to $y$ using the input variable $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For next examples we define the following values for $x$ and $y$:\n",
    "\n",
    "\\begin{equation}\n",
    "x = \\left[-2, -1, 0, 1, 2, 3, 4, 5\\right]\\\\\n",
    "y = \\left[-3, -1, 1, 3, 5, 7, 9, 11\\right]\\\\\n",
    "\\end{equation}\n",
    "\n",
    "The values for $x$ and $y$ have a linear relation so we can use linear regression to find (or fit) the best weights to solve the problem. Maybe, by staring long enough at these values, we can also find the relation, however it is much easier to use a computer to find the answer.\n",
    "\n",
    "If you have stared long enough or just want to know the answer, the relation between $x$ and $y$ is the following:\n",
    "\n",
    "\\begin{equation}\n",
    "y = 2x + 1\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section we will use Tensorflow to create our single neuron model and try to 'solve' the equation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2B) An implementation in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start with Tensorflow, we should first organize our input ($x$) and output ($y$) data. For this we are going to use Numpy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([-2, -1,  0,  1,  2,  3,  4,  5], dtype=np.float)\n",
    "Y = np.array([-3, -1,  1,  3,  5,  7,  9, 11], dtype=np.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we will import Tensorflow and check which version we are using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create a model using Keras, which is now part of Tensorflow. For this, we will use the Sequential class, which can stack various layers 'sequentially' after each other. We use the Dense class from Keras to create a 'fully connected' layer, which consists of a single neuron (unit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=1, input_shape=[1]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dense function is used to create layers of many fully connected neurons (logistic units). The parameters units is used to set the amount of neurons. We only use a single unit and therefore, set it to one. As this is the first 'layer' of our model, we need to tell Tensorflow what shape it can expect as an input. This is only nescesary for the first layer.\\\n",
    "\n",
    "Now that we have defined the model, we need to use the Compile() method to configure the model for training. The method requires at least two parameters, a loss function and an optimizer. The loss function is a measure for how wel the model predicts the actual value. For this example we will use the mean squared error (average of the squared difference between the predicted and the actual value of $y$). The 'learning algorithm' will try to minimize the loss my adjusting (optimizing) the parameters (weights and bias) for each step. The optimizer defines a method to perform this Optimizing  step and a common method is Gradient Descent, or in our case Stogastic Gradient Descent. This method will become more clear in section 2C.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the Fit() method to let the algorithm learn the best parameters. While Tensorflow has many smart ways to address this problem, what more or less is happing, are the following steps:\n",
    "1. calculate the prediction $\\hat y$ using the current weights of the model\n",
    "2. calculate the loss of the current values\n",
    "3. calculate the gradient of the loss function with respect to the parameters ($W$ and $b$)\n",
    "4. adjust the weights (optimize) using the gradient.\n",
    "5. repeat for the number of epochs, i.e. the number of times to go through the provided examples (dataset).\n",
    "\n",
    "If it is not yet clear what this means, it does not really matter yet. It will become more clear in section 2C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step will generate a lot of output. Just notice that the loss is indeed decreasing and 'close' the output cell by clicking to the left of this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, Y, epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use our model to 'predict' values it has never seen. This is sometimes also called inference. Let's try the value of 12. We know that it should be 25. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([12.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is the value not exactly 25?\\\n",
    "The model calculates the difference between the actual value and the predicted value and creeps slowly towards the actual value. Running the fit procedure longer will get you closer to 25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was not that hard, but it might feel dark Jedi force. Therefore, in the next section we will implement this algorithm in plain Python (with the help of Numpy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2C) What's actually happening 'under the hood'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section we gave a rough overview what Tensorflow is doing under the hood:\n",
    "\n",
    "1. calculate the prediction $\\hat y$ using the current weights of the model\n",
    "2. calculate the loss of the current values\n",
    "3. calculate the gradient of the loss function with respect to the parameters ($W$ and $b$)\n",
    "4. adjust the weights (optimize) using the gradient.\n",
    "5. repeat for the number of epochs, i.e. the number of times to go through the provided examples (dataset).\n",
    "\n",
    "We will no code exactly this and hopefully come to a similar result as Tensorflow.\n",
    "\n",
    "First we define the model parameters. These are the weights $W$, which is just a single value, because we only have a single input. Also we need to define the bias term $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020) # to make reproducible results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = 0.01 * np.random.randn(1) # In deep learning it is important to initialize the weights randomly. For our example, 0 would suffice.\n",
    "b = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I called the parameters $W$ and $b$ to match their corresponding official neural network terms: weights and bias. We are however, still calculating the exact same thing as the linear regression problem:\n",
    "\\begin{equation}\n",
    "y = W x + b\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will define a function that makes a prediction using our current model parameters. In deep learning terms, this is called forward pass. The variable of the predicted value is generally name $\\hat y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, W, b):\n",
    "    yhat = W * X + b\n",
    "    return yhat "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function is named forward and uses the input vector $X$ and multiplies it with the weight parameter $W$ and adds the bias term $b$. Exactly as we decribed in the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test the function with the current parameters. Again, we will input a value of 12.0 but of course, it will return gibberish as the weights are randomly initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = forward(12.0, W, b)\n",
    "yhat[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed not quite right, but we still have to train our model. Before we can do that we need to calculate the current loss. As a loss we used the mean squared error of the predicted value $\\hat y$ and the actual value $y$.\n",
    "\n",
    "\\begin{equation}\n",
    "Loss = \\frac{1}{m} \\sum_{i=1}^{m} (y - \\hat{y})^2\n",
    "\\end{equation}\n",
    "\n",
    "Hopefully the math does not scare you, but if you take the time, it is not that hard. The variabel $m$ here is the amount of examples (points in the dataset). Our $X$ holds eight values and therefore, $m=8$. When we have a $1/m$ followed by as sum ($\\sum$) over $m$ is an average of all the values that are $inside$ the summation. Here, we take the average over all $(y - \\hat{y})^2$ which is the difference between the actual value and the predicted value $\\hat y$, squared. The square is important because negative difference and a positve differnce would cancel each other out if we would not square the difference. Now that we fully understand the *mean squared error*, we can implement it in code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(yhat, Y):\n",
    "    m = len(yhat)\n",
    "    loss = 1/m * np.sum(yhat - Y)**2\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what the loss is between our previously calculated value, we can do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(yhat, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our weights are pretty much off and we need to update them. To do that we need to first calculate the gradients $\\delta$Loss/$\\delta W$ and $\\delta$Loss/$\\delta b$. Maybe your differential skills are a bit rusty. The trick is to apply the product rule. We can ignore the sums as these are linear:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\delta}{\\delta W} Loss =  \\frac{1}{m} \\sum_{i=1}^{m}  \\frac{\\delta}{\\delta W} (y - \\hat{y})^2 \\\\\n",
    "U = (y - \\hat{y}) = (y - (Wx + b)) \\\\\n",
    "\\frac{\\delta}{\\delta W} Loss = \\frac{1}{m} \\sum_{i=1}^{m}  \\frac{\\delta}{\\delta U} U^2 \\frac{\\delta}{\\delta W} -(W x + b) \\\\\n",
    "\\frac{\\delta}{\\delta W} Loss = \\frac{1}{m} \\sum_{i=1}^{m}  -2x(y - \\hat{y})\n",
    "\\end{equation}\n",
    "\n",
    "For $\\delta$Loss/$\\delta b$ we only need to repeat the last step, with respect to $b$:\n",
    "\\begin{equation}\n",
    "\\frac{\\delta}{\\delta b} Loss = \\frac{1}{m} \\sum_{i=1}^{m}  \\frac{\\delta}{\\delta U} U^2 \\frac{\\delta}{\\delta b} -(W x + b) \\\\\n",
    "\\frac{\\delta}{\\delta b} Loss = \\frac{1}{m} \\sum_{i=1}^{m}  -2(y - \\hat{y})\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement this in the 'backward pass' function. As variable names get a bit long, we will just call them $dW$ and $db$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(X, Y, yhat):\n",
    "    m = len(yhat)\n",
    "    dW = 1/m * np.sum( -2 * X * (Y - yhat))\n",
    "    db = 1/m * np.sum( -2 * (Y - yhat))\n",
    "    return (dW, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we can get the gradient of our previous test example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dW, db) = backward(12.0, 25, yhat)\n",
    "(dW, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last function we need, before we can compose our training loop is the update function. This function will 'optimize' our weights one step. This is the actual gradient descent in which we subtract (descent) the gradient from our current weights. Gradient descent is defined as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "W = W - \\alpha \\delta W\\\\\n",
    "b = b - \\alpha \\delta b\n",
    "\\end{equation}\n",
    "\n",
    "Here we have a new parameter $\\alpha$ which is called the learning rate. We will set it to 0.01. Our code for the update function is as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(W, b, dW, db, learning_rate = 0.01):\n",
    "    W = W - learning_rate * dW\n",
    "    b = b - learning_rate * db\n",
    "    return (W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To update our curent model paramters we simply do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(W, b) = update(W, b, dW, db)\n",
    "(W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we have updated our weights for the first time. To improve the weights, we have to repeat this process many times. For this we will write a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 500\n",
    "\n",
    "# a reset for W and b\n",
    "np.random.seed(2020)\n",
    "W = 0.01 * np.random.randn(1)\n",
    "b = 0\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    yhat = forward(X, W, b)\n",
    "    l = loss(Y, yhat)\n",
    "    dW, db = backward(X, Y, yhat)\n",
    "    W, b = update(W, b, dW, db)\n",
    "    if i % 100 == 0:\n",
    "        print(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are in luck and the loss, i.e. the difference between our model prediction and the actual value, is decreasing. How would we now predict, when we input a value of 12.0?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward(12.0, W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope that the magic box in Tensorflow is now a bit more clear. In the next section we will use our new knowledge for binary logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) From Linear Regression to Binary Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3A) What is the difference?\n",
    "The differences between linear regressions and a logistic regressions are not major. There are two differences from the previous code we created. First, our linear regression model only had a single feature, which we inputted with $x$, meaning that we only had a single weight. In logistic regression, you generally input more than one feature, and each will have its own weight. This will change the previous simple multiplication to a matrix multiplication (dot product). Secondly, we will add a so called *activation function* to map this value between 0 or 1. Let's remind ourselves again of our simple model:\n",
    "\n",
    "<img src=\"assets/logisticunit.png\" alt=\"Image\" width=\"600\" style=\"display: block; margin: 0 auto\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By convention (from what I have understood) in Tensorflow, the input vector has columns for features, and rows for examples. If we would have 2 datapoints the input matrix would look like this:\n",
    "\n",
    "\\begin{equation}\n",
    "X = \\left( \n",
    "\\begin{matrix}\n",
    "f^1_1 & f^2_1 & f^3_1 & f^4_1 \\\\\n",
    "f^1_2 & f^2_2 & f^3_2 & f^4_2 \n",
    "\\end{matrix} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "The superscript shows the feature number, the subscript indicates the example.\n",
    "\n",
    "Each of these inputs are associated with their own weights. The node itself has two explict operations. The first is the dot product between the weights vector and the input vector. The second is the sigmoid funtion. The weight vector $W$ in this example has four weights:\n",
    "\n",
    "\\begin{equation}\n",
    "X = \\left( \n",
    "\\begin{matrix}\n",
    "W^1 \\\\\n",
    "W^2 \\\\\n",
    "W^3 \\\\\n",
    "W^4\n",
    "\\end{matrix} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "In the node we first compute the linear part:\n",
    "\n",
    "\\begin{equation}\n",
    "Z = W X + b\n",
    "\\end{equation}\n",
    "\n",
    "For our example this will look like this:\n",
    "\n",
    "\\begin{equation}\n",
    "Z = \\left( \n",
    "\\begin{matrix}\n",
    "W^1 \\\\\n",
    "W^2 \\\\\n",
    "W^3 \\\\\n",
    "W^4\n",
    "\\end{matrix} \\right) \\left( \n",
    "\\begin{matrix}\n",
    "f^1_1 & f^2_1 & f^3_1 & f^4_1 \\\\\n",
    "f^1_2 & f^2_2 & f^3_2 & f^4_2 \n",
    "\\end{matrix} \\right) + b = \\left(\n",
    "\\begin{matrix}\n",
    "W^1 f^1_1 + W^2 f^2_1 + W^3 f^3_1 + W^4 f^4_1 + b\\\\\n",
    "W^1 f^1_1 + W^2 f^2_1 + W^3 f^3_1 + W^4 f^4_1 + b\n",
    "\\end{matrix}\n",
    "\\right) = \\left( \n",
    "\\begin{matrix}\n",
    "z_1 \\\\\n",
    "z_2 \n",
    "\\end{matrix} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "Notice that the result is only a single value for each example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all values of $x$, which can be from $-\\infty$ to $+\\infty$ (all real numbers), the sigmoid function maps $x$ between 0 and 1. Only values for $x$ close to zero matter, as these are in the 'linear regime'. Very large, or very small are only clipped to 1 and 0 respectively. The mathematical definition of the sigmoid is:\n",
    "\n",
    "\\begin{equation}\n",
    "A = \\frac{1}{1 + e^{-Z}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all there is to the logistic unit. The sigmoid function gives the node its non-linear character. Many of these units together can do almost magical things. First we will make our first logistic regression model in Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3B) An implementation in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start we first need some data to do a logistic regression. I downloaded the titantic dataset from Azeem Bootwala from Kaggle to have a play for this example. It can be downloaded from here:\\\n",
    "https://www.kaggle.com/azeembootwala/titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('titanic/train_data.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, always inspect the columns and data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I expect that Azeem did not save the set using index=False option and therefore we have a 'Unnamed: 0' column. This one is redundant with our current index so we can remove it. Also the PassengerId is not useful for our model, lets remove that column too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Unnamed: 0', 'PassengerId'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets observe some random examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Azeem already did some preprocessing. The target variable $Y$ is 'Survived', all other columns are features.\\\n",
    "A short description:\n",
    "- Sex: 0 or 1 -> male or female\n",
    "- age: value rescaled between 0 and 1\n",
    "- fare: ticket price rescaled between 0 and 1\n",
    "- Pclass_1 .. Pclass_3: One-hot encoded Passenger class\n",
    "- family size: rescaled value between 0 and 1 of family size.\n",
    "- title_1 .. title_4: mr, mrs, master, miss one-hot encoded\n",
    "- emb_1 .. emb_3: Embark location one-hot encoded. \n",
    "\n",
    "In total we will have 14 features.\n",
    "\n",
    "For this example, the data will suffice and I will not go into detail on how this data has become what it is. Honestly, I do not know myself and have just downloaded it ;-)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets put these variables in the format we defined before ($X$ and $Y$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['Survived'].to_numpy()\n",
    "X = df.iloc[:,1:].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable $Y$ is the label if they survived and we had 792 examples. So the shape will be $(m, 1)$ where $m$ = 792:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy omits the 1 for the single column as it is redundant. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For X, we expect a shape of ($m$, 14):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data, lets create the model in Tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(units=1, input_shape=[14], activation='sigmoid'),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model in Tensorflow is very similar to our linear regression model. The input has changed from 1 to 14 features and we added an activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To configure the model, we again do a compile. For this example I changed the loss into 'binary_cross_entropy'. This is another loss function which works better for binary logistic regression problems. If you are interested in the inner workings I recommend wikipedia. \\\n",
    "I also added the metric 'accuracy' to calculate the accuracy for each epoch. This should improve as we train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history = model.fit(X, Y, epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this was all to training a binary logistic classifier in Tensorflow. We achieved an accuracy of ~80% which is not too bad for the effort we put in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the weights $W$ and the bias $b$ from our model. These values we can later compare to our own implementation of the logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_tf, b_tf = [x.numpy() for x in model.weights]\n",
    "W_tf, b_tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the loss if we like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "ax.plot(np.arange(500), train_history.history['loss'], 'b-', label='loss')\n",
    "xlab, ylab = ax.set_xlabel('epoch'), ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't mind the nice XKCD wobble :-). It is quite impressive how few steps are required to get to such a result. In the final section we will unveal the Dark Jedi arts being performed by Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3C) What is actually happening?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, the steps required are still the same:\n",
    "1. forward pass\n",
    "2. calculate loss\n",
    "3. backward pass\n",
    "4. update weights\n",
    "5. repeat\n",
    "\n",
    "Al steps have minor changes. The forward pass will be a more general dot product and we need to add the activation function. This final activation functions gives a value between 0 and 1, we need to round this to an actual value of 0 or 1. The loss function is binary cross entropy, which is of course different from the mean square error. The backward pass will calculate the gradient of the loss function with respect to $W$ and $b$. As our loss function changed, we will have different differentials. The update weights function is unchanged, the final loop is also very similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Numpy, and in math in general as far as I know, the dot product needs the shapes of the vectors (and matrices) to be compatible:\n",
    "\n",
    "\\begin{equation}\n",
    "X Y = X_{i,j} Y_{j, k}\n",
    "\\end{equation}\n",
    "\n",
    "This means that the number of columns of $X$ must be equal to the number of rows in $Y$. To make ourselves a bit easier, we will Transpose our input vector $X$ and flip the vector. This will result that the rows will be features and the columns will be examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets first get two rows from our dataset to do test calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtry = X[ :, :2]\n",
    "Ytry = Y[:2]\n",
    "Xtry.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ytry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets define our weights. As we have 14 features, our vector $W$ will have 14 values. The bias is a constant for the whole node, and is only a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)\n",
    "W = 0.01 * np.random.randn(14)\n",
    "b = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we need to calculate the sigmoid function:\n",
    "\\begin{equation}\n",
    "A = \\frac{1}{1 + e^{-Z}}\n",
    "\\end{equation}\n",
    "lets create it in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(np.array([-100, 0, 0.1, 1000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets redefine our forward function to do the dot product and the activation function. The forward pass is defined in two steps:\n",
    "1. $Z = W X + b$\n",
    "2. $\\sigma(Z)$\n",
    "\n",
    "Note that $W X$ is a dot product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, W, b):\n",
    "    Z = np.dot(W.T, X) + b\n",
    "    A = sigmoid(Z)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = forward(Xtry, W, b)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have actual predictions, we can write our loss function to measure how well our predictions are. This is done with the binary cross entropy, for which I will simply give the equation. \n",
    "\\begin{equation}\n",
    "loss = -\\frac{1}{m}\\sum_{i=1}^{m}y\\log(A)+(1-y)\\log(1-A)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might happen for the first iteration that we try to calculate a $\\log(0)$ which is of course not defined. To avoid the warning, we will add a tiny value to our loss. As it is super small, the difference is not noticable, but does help suppress the warning. One less warning a day keeps the ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(A, Y, epsilon = 1e-15):\n",
    "    m = len(A)\n",
    "    l = -1/m * np.sum( Y * np.log(A + epsilon) + (1 - Y) * np.log(1 - A + epsilon))\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(A, Ytry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is the backwards pass. For this, We would need to differentiate the Loss function with $W$ and $b$. Not to  bore you guys, I have provided these functions:\n",
    "\\begin{equation}\n",
    " \\frac{\\partial loss}{\\partial W} = \\frac{1}{m} \\sum_{i=1}^m  X(A - Y)^T \\\\\n",
    " \\frac{\\partial loss}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (A - y)\n",
    "\\end{equation}\n",
    "In Python, this looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(X, Y, A):\n",
    "    m = len(yhat)\n",
    "    dW = 1/m * np.dot(X, (A - Y).T)\n",
    "    db = 1/m * np.sum(A - Y) \n",
    "    return (dW, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dW, db) = backward(Xtry, Ytry, A)\n",
    "dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost there, next we need to update the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(W, b, dW, db, learning_rate = 0.01):\n",
    "    W = W - learning_rate * dW\n",
    "    b = b - learning_rate * db\n",
    "    return (W, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update(W, b, dW, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare the results, we can calculate the accuracy. However our activation function returns a probability between 0 and 1. By definition, values <= 0.5 are rounded to 0 and values > 0.5 are rounded to 1. This is slightly different from the round function so we will make our own function for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def roundValue(A):\n",
    "    return np.uint8( A > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = roundValue(A)\n",
    "yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(yhat, Y):\n",
    "    return round(np.sum(yhat==Y) / len(yhat) * 1000) / 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now finally lets implement the final loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 8000\n",
    "lr = 0.01\n",
    "\n",
    "# Lets just reset W and b\n",
    "np.random.seed(2020)\n",
    "W = 0.01 * np.random.randn(14)\n",
    "b = 0\n",
    "\n",
    "losses, acces = [], []\n",
    "for i in range(num_iterations):\n",
    "    A = forward(X, W, b)\n",
    "    l = loss(Y, A)\n",
    "    yhat = roundValue(A)\n",
    "    acc = accuracy(yhat, Y)\n",
    "    dW, db = backward(X, Y, A)\n",
    "    W, b = update(W, b, dW, db, learning_rate=lr)\n",
    "    losses.append(l)\n",
    "    acces.append(acc)\n",
    "    if i % 1000 == 0:\n",
    "        print('loss:', l, f'\\taccuracy: {accuracy(yhat, Y)}%') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the loss function of our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "ax.plot(np.arange(len(losses)), losses, 'b-', label='loss')\n",
    "xlab, ylab = ax.set_xlabel('epoch'), ax.set_ylabel('loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "ax.plot(np.arange(len(acces)), acces, 'b-', label='accuracy')\n",
    "xlab, ylab = ax.set_xlabel('epoch'), ax.set_ylabel('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Round up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this was it for the tutorial on Logistic regression. Hopefully you got an idea on how Logistic reggression work and that Tensorflow is not only black magic. I found it a great excersise to write these from scratch and as you have seen, it is that difficult either.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So long and thanks for all the fish!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
